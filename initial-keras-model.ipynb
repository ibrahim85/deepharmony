{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Initial Keras model\n",
    "\n",
    "Simple keras model to make sure that I know how everything can fit together.\n",
    "\n",
    "I threw together the `separateVoices` code and the preprocessing code (changed to match `separateVoices` naming convention) and ran it through a keras model... not any keras model we care about though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM\n",
    "\n",
    "import music21 as m21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "song = m21.converter.parse('data/bach-chorales/bach-1.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"separateVoices may put the same note in two different voices...\n",
    "(This is probably not a problem: these Bach chorals don't appear to have rests)\n",
    "\n",
    "when getElementsByOffset is called in a situation like:\n",
    "1   2   3   4\n",
    "XXX XXX\n",
    "YYY YYY\n",
    "    ZZZ\n",
    "WWW WWW\n",
    "\n",
    "Note W will be added to both the Tenor and the Bass part...\n",
    "\"\"\"\n",
    "\n",
    "def separateVoices(song):\n",
    "    timeSig = song.recurse().getElementsByClass('TimeSignature')[0]\n",
    "    tempo = song.recurse().getElementsByClass('MetronomeMark')[0]\n",
    "\n",
    "    sepVoices = m21.stream.Score()\n",
    "\n",
    "    parts = []\n",
    "    for voice in ['Soprano','Alto','Tenor','Bass']:\n",
    "        part = m21.stream.Part()\n",
    "        part.partName = voice\n",
    "        part.append(timeSig)\n",
    "        part.append(tempo)\n",
    "        #part.append(m21.clef.TrebleClef() #just for readability\n",
    "        parts.append(part)\n",
    "\n",
    "    sepVoices.append(parts)\n",
    "\n",
    "    # takes Chord, returns list of component Notes\n",
    "    def chord2notes(chord):\n",
    "        return [m21.note.Note(pitch, duration=chord.duration) for pitch in chord.pitches]\n",
    "\n",
    "    # Replace chords with notes\n",
    "    def removeChords(stream):\n",
    "        import copy\n",
    "        newStream = copy.deepcopy(stream)\n",
    "        for noteOrChord in newStream.flat.notes:\n",
    "            if noteOrChord.isChord:\n",
    "                offset = noteOrChord.offset\n",
    "                newStream.remove(noteOrChord, recurse=True)   \n",
    "\n",
    "                for note in chord2notes(noteOrChord):\n",
    "                    newStream.insert(offset,note)\n",
    "        return newStream\n",
    "\n",
    "    #Remove all chords\n",
    "    chordless = removeChords(song)\n",
    "\n",
    "    # At each note, identify and separate the voices\n",
    "    for note in chordless.flat.notes:\n",
    "#     for note in chordless.flat.getElementsByOffset(0,11, classList=['Note']):\n",
    "        simulNotes = list(chordless.flat.getElementsByOffset(note.offset, mustBeginInSpan=False, classList=['Note']))\n",
    "        simulNotes.sort(key=lambda x: x.pitch, reverse=True)\n",
    "        \n",
    "\n",
    "        for part, newnote in zip(parts, simulNotes):                \n",
    "            if newnote.id not in [n.id for n in part.flat.notes] and note==newnote:\n",
    "                part.insert(newnote.offset,newnote)\n",
    "                \n",
    "    return sepVoices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A0 is 21 and C8 is 108\n",
    "HOLD_NOTE = 20\n",
    "SILENCE = 19\n",
    "\n",
    "\n",
    "# Converts note or chord to a midi pitch (int)\n",
    "def pitch(note):\n",
    "    try:\n",
    "        #... Should probably change this part\n",
    "        return note.pitches[0].midi # If it's a chord, get top note\n",
    "    except AttributeError:\n",
    "        return note.pitch.midi # if it's not a chord\n",
    "\n",
    "\n",
    "# music21.Stream -> [int...]\n",
    "# Converts a 'voice' (with no chords) to series of pitches\n",
    "def makePitchArray(voice):\n",
    "    # Divisions are in quarter notes... we may want to change this\n",
    "    array = []\n",
    "    current_note = 0\n",
    "    for time in range(0, int(voice.duration.quarterLength)):\n",
    "        # Figure out current note\n",
    "        notes = voice.flat.getElementsByOffset(time, mustBeginInSpan=False, classList=['Note'])\n",
    "        if len(notes) == 0:\n",
    "            array.append(SILENCE)\n",
    "        else:\n",
    "            note = notes[0]\n",
    "            if note.offset > time-1:\n",
    "                # The note started in the past quarternote\n",
    "                array.append(note.pitch.midi)\n",
    "            else:\n",
    "                array.append(HOLD_NOTE)\n",
    "    return array\n",
    "\n",
    "# Converts pitch number to a tuple with \"one hot encoding\"\n",
    "LOWEST_PITCH=SILENCE\n",
    "HIGHEST_PITCH=108 # C8\n",
    "def pitchToTuple(pitch):\n",
    "    list = [0] * (HIGHEST_PITCH-LOWEST_PITCH)\n",
    "    list[pitch-LOWEST_PITCH] = 1\n",
    "    return tuple(list)\n",
    "\n",
    "def tupleToPitch(tuple):\n",
    "    index = max(enumerate(tuple), key=lambda x: x[1])[0]\n",
    "    return LOWEST_PITCH + index\n",
    "\n",
    "def pitchToStream(pitch_array):\n",
    "    stream = m21.stream.Stream()\n",
    "    for pitch in pitch_array:\n",
    "        if pitch == HOLD_NOTE:\n",
    "            stream[-1].quarterLength += 1\n",
    "        elif pitch != SILENCE:\n",
    "            stream.append(m21.note.Note(pitch))\n",
    "    return stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "voices = separateVoices(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess(song):\n",
    "    voices = separateVoices(song)\n",
    "    tuple_arrays = []\n",
    "    for voice in voices:\n",
    "        pitch_array = makePitchArray(voice)\n",
    "        tuple_arrays.append(list(map(pitchToTuple, pitch_array)))\n",
    "    return tuple_arrays\n",
    "\n",
    "preprocessed = preprocess(song)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Keras model \n",
    "A naively stupid deep learning model to predict the next note.\n",
    "\n",
    "Most of the keras code comes straight off of the \"Getting Started\" guide on keras.io."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "207/207 [==============================] - 0s - loss: 4.4420 - acc: 0.0242         \n",
      "Epoch 2/10\n",
      "207/207 [==============================] - 0s - loss: 4.4116 - acc: 0.1353     \n",
      "Epoch 3/10\n",
      "207/207 [==============================] - 0s - loss: 4.3821 - acc: 0.2126     \n",
      "Epoch 4/10\n",
      "207/207 [==============================] - 0s - loss: 4.3527 - acc: 0.4541     \n",
      "Epoch 5/10\n",
      "207/207 [==============================] - 0s - loss: 4.3236 - acc: 0.4928     \n",
      "Epoch 6/10\n",
      "207/207 [==============================] - 0s - loss: 4.2934 - acc: 0.4928     \n",
      "Epoch 7/10\n",
      "207/207 [==============================] - 0s - loss: 4.2644 - acc: 0.4928     \n",
      "Epoch 8/10\n",
      "207/207 [==============================] - 0s - loss: 4.2340 - acc: 0.4928     \n",
      "Epoch 9/10\n",
      "207/207 [==============================] - 0s - loss: 4.2031 - acc: 0.4928     \n",
      "Epoch 10/10\n",
      "207/207 [==============================] - 0s - loss: 4.1729 - acc: 0.4928     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11396aac8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PITCH_LEN = len(pitchToTuple(SILENCE))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(output_dim=PITCH_LEN, input_dim=PITCH_LEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(output_dim=PITCH_LEN))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# Train to match the next note\n",
    "data = np.array(preprocessed[0][:-1])\n",
    "labels = np.array(preprocessed[0][1:])\n",
    "model.fit(data, labels, nb_epoch=10, batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the stupid keras model to predict  the next pitch given G2#\n",
    "# It should predict SILENCE\n",
    "tupleToPitch(model.predict(np.array([pitchToTuple(31)]))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tupleToPitch(model.predict(np.array([pitchToTuple(20)]))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A slightly less naive and foolish keras model\n",
    "\n",
    "(Not working yet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PITCH_LEN = len(pitchToTuple(SILENCE))\n",
    "ARBITRARY_DIMENSIONS=12\n",
    "MIN_SONG_LENGTH=200 #TODO figure out/look up\n",
    "\n",
    "model = Sequential([\n",
    "        Embedding(PITCH_LEN, ARBITRARY_DIMENSIONS, input_length=MIN_SONG_LENGTH),\n",
    "        LSTM(ARBITRARY_DIMENSIONS, return_sequences=True),\n",
    "        # Dropout(0.5),\n",
    "        TimeDistributed(Dense(89)),\n",
    "        Activation('sigmoid')\n",
    "        ])\n",
    "# Binary crossentropy?\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/5000 [00:00<02:19, 35.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing files\n",
      "Loading files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 733/5000 [01:38<36:19,  1.96it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "def loadSongs():\n",
    "    print('Listing files')\n",
    "    files = glob.glob('data/bach-chorales/*.mid')\n",
    "    print('Loading files')\n",
    "    songs = [m21.converter.parse(file) for file in tqdm(files)]\n",
    "    print('Calculating minimum song length')\n",
    "    min_length = min([song.duration.quarterLength for song in tqdm(songs)])\n",
    "    print('Separating voices')\n",
    "    voices_by_song = [separateVoices(song) for song in tqdm(songs)]\n",
    "    print('Getting soprano part (input data)')\n",
    "    soprano_pitches = [makePitchArray(voices[0])[:min_length] for voices in voices_by_song]\n",
    "    print('Getting bass part (output labels)')\n",
    "    bass_tuples = [ list(map(pitchToTuple, makePitchArray(voices[3])))[:min_length] for voices in voices_by_song]\n",
    "    \n",
    "    return (soprano_pitches, bass_tuples, min_length)\n",
    "\n",
    "loadedData = loadSongs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = loadedData[0]\n",
    "labels = loadedData[1]\n",
    "min_song_length=loadedData[2]\n",
    "\n",
    "model.fit(data, labels, nb_epoch=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def predict_harmony(melody_pitches_so_far):\n",
    "#     return tupleToPitch([model.predict([melody_pitches_so_far])])\n",
    "\n",
    "\n",
    "# list(map(predict_harmony, data))\n",
    "\n",
    "[tupleToPitch(n) for n in model.predict(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
